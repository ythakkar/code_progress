{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## libraries\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import scipy.spatial.distance\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## data loading\n",
    "qa = np.genfromtxt(\"uid_qa.txt\", delimiter = \",\", names = True, dtype = [('int64'), ('int64'), ('int64'), ('U256'), ('U128'),])\n",
    "fe = np.genfromtxt(\"uid_pre_elim.txt\", delimiter = \",\", names = True, dtype = [('int64'), ('int64'), ('int64'), ('U256'), ('U256'),])\n",
    "ftd = np.genfromtxt(\"face_id_descr.txt\", delimiter = \";\", skip_header = 1 , usecols = np.arange(0,2), dtype = [('U16'), ('U2056')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ftd.dtype.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#changing or adding names for the dtypes\n",
    "fe.dtype.names = ('uniqueID', 'bn', 'qn', 'pre_que', 'curr_elim')\n",
    "ftd.dtype.names = ('img_id', 'description')\n",
    "len(ftd['img_id'])\n",
    "len(set(ftd['img_id']))\n",
    "\n",
    "#fe.dtype.names\n",
    "#fe[971]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## first convert strings to list of rows equal to rows in fe\n",
    "def mk_list(str_vector):\n",
    "    des_byid = str_vector\n",
    "    all_x = []\n",
    "    for i in des_byid:\n",
    "        j = i.split()\n",
    "        #if len(j) > 0:\n",
    "        all_x.append(j)\n",
    "\n",
    "    return all_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15131"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_x = mk_list(fe['pre_que'])\n",
    "len(all_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 43, 114, 115, 117, 118, 130, 131, 134, 135, 136, 137, 138, 139, 140, 234, 358, 408, 537, 607, 636, 674, 923, 928, 929, 931, 932, 949, 1078, 1079, 1196, 1250, 1251, 1258, 1264, 1274, 1364, 1433, 1493, 1494, 1545, 1600, 1753, 1907, 1996, 2045, 2077, 2095, 2103, 2105, 2106, 2167, 2168, 2169, 2201, 2308, 2312, 2413, 2455, 2535, 2536, 2542, 2657, 2681, 2706, 2772, 2773, 2774, 2778, 2779, 2781, 2782, 2783, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2798, 2873, 2876, 2902, 2942, 2949, 2965, 3079, 3201, 3207, 3258, 3308, 3411, 3416, 3426, 3439, 3440, 3444, 3445, 3446, 3449, 3496, 3506, 3507, 3549, 3600, 3602, 3610, 3612, 3616, 3619, 3621, 3624, 3626, 3628, 3652, 3654, 3655, 3656, 3665, 3682, 3715, 3726, 3727, 3730, 3733, 3735, 3829, 3912, 3954, 3959, 4001, 4002, 4003, 4004, 4005, 4006, 4007, 4008, 4024, 4025, 4302, 4335, 4363, 4415, 4450, 4522, 4523, 4536, 4572, 4621, 4723, 4734, 4736, 4737, 4792, 4793, 4794, 4823, 4824, 4825, 4826, 4827, 4828, 4829, 4830, 4898, 4904, 4916, 4989, 5019, 5040, 5061, 5062, 5065, 5120, 5133, 5354, 5439, 5511, 5526, 5527, 5528, 5529, 5530, 5585, 5676, 5679, 5681, 5688, 5709, 5712, 5713, 5714, 5715, 5717, 5718, 5720, 5723, 5725, 5734, 5754, 5782, 5918, 5977, 5997, 6104, 6106, 6113, 6117, 6320, 6346, 6513, 6575, 6595, 6626, 6627, 6628, 6629, 6633, 6783, 6909, 6939, 6955, 7096, 7144, 7187, 7413, 7472, 7484, 7497, 7662, 7714, 7715, 7777, 7860, 7865, 7871, 7873, 7875, 7880, 7885, 7890, 7891, 7893, 7894, 7895, 7897, 7900, 7902, 7905, 7930, 7999, 8040, 8067, 8108, 8223, 8233, 8234, 8239, 8324, 8400, 8401, 8410, 8493, 8576, 8589, 8756, 8780, 8850, 8852, 8894, 8899, 9005, 9151, 9295, 9528, 9530, 9535, 9601, 9635, 9681, 9696, 9697, 9878, 9879, 9976, 10132, 10140, 10224, 10228, 10231, 10245, 10261, 10354, 10357, 10358, 10359, 10457, 10467, 10479, 10481, 10489, 10506, 10515, 10540, 10600, 10635, 10636, 10637, 10638, 10639, 10643, 10707, 10708, 10728, 10729, 10788, 10812, 10890, 10975, 10976, 10977, 10987, 10993, 10995, 10996, 10998, 11001, 11003, 11025, 11104, 11117, 11195, 11196, 11228, 11231, 11243, 11244, 11245, 11392, 11418, 11430, 11461, 11463, 11509, 11553, 11600, 11661, 11672, 11683, 11688, 11696, 11698, 11708, 11709, 11716, 11720, 11910, 11924, 11936, 11954, 11955, 11965, 11967, 11972, 11975, 11982, 11985, 12104, 12133, 12174, 12203, 12231, 12232, 12240, 12241, 12242, 12243, 12337, 12428, 12432, 12437, 12519, 12525, 12593, 12690, 12730, 12731, 12778, 12779, 12788, 12797, 12798, 12842, 12875, 12878, 13011, 13017, 13037, 13054, 13058, 13059, 13063, 13071, 13072, 13083, 13084, 13093, 13094, 13162, 13189, 13191, 13217, 13221, 13245, 13351, 13402, 13405, 13518, 13558, 13564, 13645, 13680, 13684, 13690, 13692, 13790, 13797, 13799, 13800, 13894, 14009, 14010, 14011, 14012, 14013, 14016, 14143, 14167, 14189, 14190, 14191, 14192, 14193, 14194, 14195, 14197, 14198, 14199, 14200, 14201, 14202, 14204, 14205, 14206, 14207, 14208, 14209, 14210, 14211, 14212, 14251, 14266, 14326, 14364, 14416, 14617, 14670, 14672, 14673, 14677, 14870, 14893, 14916, 14932, 14953, 14954, 14955, 14957, 14959, 15071, 15098, 15124, 15125, 15127, 15128]\n"
     ]
    }
   ],
   "source": [
    "##check blanks\n",
    "## As there are some rows where there is only one image or blank/erronous question. We check that and save those indices.\n",
    "## create blank list\n",
    "#good_index = ['True']*len(all_x)\n",
    "\n",
    "bl =[]\n",
    "\n",
    "for img,q in zip(enumerate(all_x), mk_list(qa['que'])):\n",
    "    if (len(img[1]) <2 or len(q) < 2) and img[0] not in bl:\n",
    "        bl.append(img[0])\n",
    "print(bl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15131"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_x)\n",
    "len(mk_list(qa['que']))\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Now for each row in the list above, find description and add it\n",
    "all_i_txt = []\n",
    "original_ind = []\n",
    "bl_ind = []\n",
    "\n",
    "for rown in range(len(all_x)):\n",
    "    if rown in bl:\n",
    "        bl_ind.append(rown)\n",
    "    else:\n",
    "        #print(all_x.index)\n",
    "        int_i_txt = []\n",
    "        for img in all_x[rown]:\n",
    "            if img in ftd['img_id']:\n",
    "                d = str(ftd['description'][list(ftd['img_id']).index(img)])\n",
    "                int_i_txt.append(str(d))\n",
    "        original_ind.append(rown)\n",
    "        all_i_txt.append(int_i_txt)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14644\n",
      "14644\n",
      "487\n",
      "487\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#print check\n",
    "#print(type(all_i_txt[0]))\n",
    "#print(len(all_i_txt))\n",
    "#print(all_i_txt[971])\n",
    "#print(all_i_txt[1473])\n",
    "print(len(original_ind))\n",
    "print(len(all_i_txt))\n",
    "print(len(bl_ind))\n",
    "print(len(bl))\n",
    "print(bl == bl_ind)\n",
    "#print(original_ind)\n",
    "#for it in original_ind:\n",
    "#    print(qa['ans'][it])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## For each question, find the synonyms for all the adjectives, NN, NNS and CD. Then add that list of synonyms in the \n",
    "##column called question \n",
    "\n",
    "new_table_fd = [] ## for frequency distribution\n",
    "new_que_syn = []\n",
    "for rown in qa['que'] :\n",
    "    #print(rown[-2])\n",
    "    #new_row = []\n",
    "    sents = nltk.sent_tokenize(rown.strip())\n",
    "    \n",
    "    lw = [nltk.word_tokenize(s) for s in sents]\n",
    "    #p =[nltk.pos_tag(w) for w in lw]\n",
    "    x = [kv[0] for w in lw for kv in nltk.pos_tag(w) if kv[1] in ['JJ', 'NN', 'NNS', 'CD']]\n",
    "    fd = [kv[1] for w in lw for kv in nltk.pos_tag(w)]\n",
    "    #y = [wordnet.synsets(words) for words in x]\n",
    "    syn = [j.name() for words in x for i in wordnet.synsets(words) for j in i.lemmas()]\n",
    "    new_que_syn.append(list(set(syn)))\n",
    "    new_table_fd.extend([kv[0] for w in lw for kv in nltk.pos_tag(w)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## add question and answer to each data point also convert the list of synonyms in the question to a string\n",
    "for it in range(len(all_i_txt)):\n",
    "    all_i_txt[it].extend([' '.join(map(str, new_que_syn[it])), qa['ans'][it].strip()])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['face_fungus byssus beard whiskers', 'No']\n",
      "['wear tire_out get_into looking_glass wearable methamphetamine wearing tire meth specs fag_out don spectacles have_on wear_off weary fall_apart ice glasses glaze_over Methedrine trash deoxyephedrine glassful habiliment wear_down glass_in article_of_clothing bear wear_upon methamphetamine_hydrochloride break assume spyglass glass wear_thin endure glaze drinking_glass chicken_feed hold_out clothing put_on fatigue wear_out vesture bust fag jade field_glass crank eyeglasses outwear chalk glass_over shabu', 'Yes']\n"
     ]
    }
   ],
   "source": [
    "## check addition of above objects\n",
    "for i in range(len(all_i_txt[0:2])):\n",
    "    print(all_i_txt[i][-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for i in set(bl):\n",
    "#    print(all_i_txt[i][-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for rown,qs in zip(all_i_txt[0:2],new_que_syn[0:2]):\n",
    "#    rown[-2] = qs\n",
    "#    print(rown[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('?', 10645),\n",
       " ('person', 9149),\n",
       " ('the', 7885),\n",
       " ('have', 5230),\n",
       " ('Is', 3951),\n",
       " ('hair', 3730),\n",
       " ('a', 3561),\n",
       " ('Does', 3308),\n",
       " ('is', 3010),\n",
       " ('this', 2314),\n",
       " ('does', 2251),\n",
       " ('glasses', 2025),\n",
       " ('wearing', 1795),\n",
       " ('they', 1209),\n",
       " ('shirt', 1146),\n",
       " ('he', 937),\n",
       " ('wear', 870),\n",
       " ('she', 798),\n",
       " ('on', 751),\n",
       " ('their', 727),\n",
       " ('long', 620),\n",
       " ('white', 584),\n",
       " ('beard', 555),\n",
       " ('her', 517),\n",
       " ('you', 512),\n",
       " ('black', 510),\n",
       " ('facial', 470),\n",
       " ('short', 459),\n",
       " ('do', 452),\n",
       " ('blonde', 451),\n",
       " ('Do', 449),\n",
       " ('male', 421),\n",
       " ('are', 415),\n",
       " ('asian', 396),\n",
       " ('life', 383),\n",
       " ('Are', 367),\n",
       " ('it', 349),\n",
       " (\"'s\", 346),\n",
       " ('in', 317),\n",
       " ('undershirt', 314),\n",
       " ('female', 311),\n",
       " ('man', 295),\n",
       " ('girl', 289),\n",
       " ('tall', 285),\n",
       " ('dark', 278),\n",
       " ('boy', 267),\n",
       " ('his', 253),\n",
       " ('of', 220),\n",
       " ('woman', 216),\n",
       " ('smiling', 215),\n",
       " ('back', 199),\n",
       " ('up', 191),\n",
       " ('say', 181),\n",
       " ('down', 174),\n",
       " ('head', 173),\n",
       " ('with', 172),\n",
       " ('ponytail', 165),\n",
       " ('brown', 161),\n",
       " ('face', 159),\n",
       " ('an', 158),\n",
       " ('light', 156),\n",
       " ('bangs', 152),\n",
       " ('pulled', 148),\n",
       " ('Asian', 144),\n",
       " ('curly', 140),\n",
       " ('persons', 140),\n",
       " ('necklace', 139),\n",
       " ('your', 137),\n",
       " ('old', 135),\n",
       " ('skin', 132),\n",
       " ('blue', 131),\n",
       " ('collar', 127),\n",
       " ('row', 119),\n",
       " ('IS', 115),\n",
       " ('tied', 110),\n",
       " ('blond', 107),\n",
       " ('under', 107),\n",
       " ('has', 105),\n",
       " ('young', 103),\n",
       " ('scarf', 99),\n",
       " ('see', 97),\n",
       " ('than', 94),\n",
       " ('word', 93),\n",
       " ('red', 92),\n",
       " ('visible', 90),\n",
       " ('that', 88),\n",
       " ('mustache', 86),\n",
       " (\"'\", 84),\n",
       " ('PERSON', 78),\n",
       " ('Can', 74),\n",
       " ('hijab', 74),\n",
       " ('to', 73),\n",
       " ('who', 72),\n",
       " ('A', 71),\n",
       " ('yellow', 69),\n",
       " ('color', 68),\n",
       " ('earrings', 67),\n",
       " ('there', 65),\n",
       " ('green', 64),\n",
       " ('words', 62),\n",
       " ('pony', 60),\n",
       " ('collared', 60),\n",
       " ('look', 60),\n",
       " ('impact', 59),\n",
       " ('parted', 59),\n",
       " ('t-shirt', 58),\n",
       " ('THIS', 57),\n",
       " ('underneath', 57),\n",
       " ('tail', 57),\n",
       " ('like', 56),\n",
       " ('older', 55),\n",
       " ('caucasian', 55),\n",
       " ('forehead', 55),\n",
       " ('says', 54),\n",
       " ('one', 54),\n",
       " ('or', 53),\n",
       " ('ears', 52),\n",
       " ('top', 52),\n",
       " ('fat', 51),\n",
       " ('ARE', 51),\n",
       " ('indian', 51),\n",
       " ('side', 51),\n",
       " ('eyes', 48),\n",
       " ('writing', 47),\n",
       " (\"'life\", 47),\n",
       " ('He', 47),\n",
       " ('THE', 47),\n",
       " ('covering', 46),\n",
       " ('descent', 46),\n",
       " ('guy', 44),\n",
       " ('big', 43),\n",
       " ('middle', 42),\n",
       " ('very', 42),\n",
       " ('something', 41),\n",
       " ('tshirt', 41),\n",
       " ('correct', 41),\n",
       " ('grey', 41),\n",
       " ('I', 41),\n",
       " ('shoulders', 37),\n",
       " ('2nd', 37),\n",
       " ('shirts', 37),\n",
       " ('can', 37),\n",
       " ('lipstick', 36),\n",
       " ('NA', 36),\n",
       " ('looking', 35),\n",
       " ('any', 35),\n",
       " ('left', 35),\n",
       " ('from', 34),\n",
       " ('round', 34),\n",
       " ('its', 34),\n",
       " ('headscarf', 33),\n",
       " ('small', 33),\n",
       " ('part', 33),\n",
       " ('bun', 33),\n",
       " ('Life', 32),\n",
       " ('large', 32),\n",
       " ('neck', 32),\n",
       " ('1st', 32),\n",
       " ('striped', 32),\n",
       " ('two', 31),\n",
       " ('over', 31),\n",
       " ('gray', 31),\n",
       " ('text', 30),\n",
       " ('hidden', 30),\n",
       " ('american', 30),\n",
       " ('right', 29),\n",
       " ('U', 29),\n",
       " ('more', 28),\n",
       " ('HAVE', 28),\n",
       " ('messy', 27),\n",
       " ('and', 27),\n",
       " ('covered', 27),\n",
       " ('shoulder', 27),\n",
       " ('skinny', 27),\n",
       " ('having', 27),\n",
       " ('skinned', 27),\n",
       " ('colored', 26),\n",
       " ('front', 26),\n",
       " ('written', 26),\n",
       " ('Caucasian', 26),\n",
       " ('GIRL', 26),\n",
       " ('photo', 25),\n",
       " ('subject', 25),\n",
       " ('HAIR', 25),\n",
       " ('african', 25),\n",
       " ('glass', 24),\n",
       " ('length', 24),\n",
       " ('i', 24),\n",
       " ('letters', 24),\n",
       " ('first', 24),\n",
       " ('purple', 24),\n",
       " ('showing', 24),\n",
       " ('bald', 23),\n",
       " ('Indian', 23),\n",
       " ('all', 22),\n",
       " ('shorter', 22),\n",
       " ('bottom', 22),\n",
       " ('straight', 22),\n",
       " ('thin', 22),\n",
       " ('loose', 22),\n",
       " ('most', 22),\n",
       " ('what', 22),\n",
       " ('not', 21),\n",
       " ('picture', 21),\n",
       " ('YOU', 21),\n",
       " ('taller', 21),\n",
       " ('inside', 21),\n",
       " ('WEARING', 20),\n",
       " ('at', 20),\n",
       " ('others', 20),\n",
       " ('longer', 20),\n",
       " ('age', 20),\n",
       " ('spectacles', 20),\n",
       " ('30', 20),\n",
       " ('happy', 20),\n",
       " ('individual', 19),\n",
       " ('pink', 19),\n",
       " ('.', 19),\n",
       " ('African', 18),\n",
       " ('Who', 18),\n",
       " ('second', 18),\n",
       " ('tan', 18),\n",
       " ('smile', 17),\n",
       " ('overweight', 17),\n",
       " ('DOES', 17),\n",
       " ('anything', 16),\n",
       " ('image', 16),\n",
       " ('target', 16),\n",
       " ('ear', 16),\n",
       " ('past', 16),\n",
       " ('my', 16),\n",
       " ('brunette', 16),\n",
       " ('eye', 15),\n",
       " ('t', 15),\n",
       " ('lips', 15),\n",
       " ('for', 15),\n",
       " ('flowers', 15),\n",
       " ('out', 14),\n",
       " ('40', 14),\n",
       " ('solid', 14),\n",
       " ('chinese', 14),\n",
       " ('nose', 14),\n",
       " ('chubby', 14),\n",
       " ('What', 13),\n",
       " ('The', 13),\n",
       " ('LIFE', 13),\n",
       " ('full', 13),\n",
       " ('years', 13),\n",
       " ('women', 13),\n",
       " ('need', 13),\n",
       " ('whether', 13),\n",
       " ('ANY', 13),\n",
       " ('moustache', 13),\n",
       " ('mohawk', 13),\n",
       " ('other', 13),\n",
       " ('tee', 13),\n",
       " ('shaven', 13),\n",
       " ('dyed', 12),\n",
       " ('clean', 12),\n",
       " ('good', 12),\n",
       " ('tie', 12),\n",
       " ('pale', 12),\n",
       " ('decent', 12),\n",
       " ('read', 12),\n",
       " ('White', 12),\n",
       " ('average', 12),\n",
       " ('hispanic', 12),\n",
       " ('darker', 12),\n",
       " ('thick', 11),\n",
       " ('muslim', 11),\n",
       " ('shaved', 11),\n",
       " ('Light', 11),\n",
       " ('combed', 11),\n",
       " ('sad', 11),\n",
       " ('lot', 11),\n",
       " ('pretty', 11),\n",
       " ('why', 11),\n",
       " ('American', 11),\n",
       " ('no', 11),\n",
       " ('how', 11),\n",
       " ('wavy', 11),\n",
       " ('eyebrows', 11),\n",
       " ('DOING', 10),\n",
       " ('6th', 10),\n",
       " ('haircut', 10),\n",
       " ('Has', 10),\n",
       " ('game', 10),\n",
       " ('cover', 10),\n",
       " ('where', 10),\n",
       " ('th', 10),\n",
       " ('frowning', 9),\n",
       " ('jewelry', 9),\n",
       " ('orange', 9),\n",
       " ('wrap', 9),\n",
       " ('line', 9),\n",
       " ('spiky', 9),\n",
       " ('oriental', 9),\n",
       " ('thing', 9),\n",
       " ('ther', 9),\n",
       " ('GLASS', 9),\n",
       " ('fair', 9),\n",
       " ('East', 9),\n",
       " ('plain', 9),\n",
       " ('lighter', 9),\n",
       " ('new', 9),\n",
       " ('below', 9),\n",
       " ('rings', 9),\n",
       " ('tone', 9),\n",
       " ('peson', 9),\n",
       " ('hanging', 9),\n",
       " ('here', 9),\n",
       " ('EYE', 9),\n",
       " ('WHO', 8),\n",
       " ('men', 8),\n",
       " ('YOUR', 8),\n",
       " ('chin', 8),\n",
       " ('flat', 8),\n",
       " ('sweater', 8),\n",
       " ('comma', 8),\n",
       " ('innershirt', 8),\n",
       " ('should', 8),\n",
       " ('name', 8),\n",
       " ('east', 8),\n",
       " ('be', 8),\n",
       " ('balding', 8),\n",
       " ('camera', 8),\n",
       " ('high', 8),\n",
       " ('cut', 8),\n",
       " ('turtleneck', 8),\n",
       " (']', 8),\n",
       " ('did', 8),\n",
       " ('sides', 8),\n",
       " ('eyeglasses', 8),\n",
       " ('blank', 8),\n",
       " ('really', 8),\n",
       " (\"'the\", 8),\n",
       " ('hairs', 8),\n",
       " ('lettering', 8),\n",
       " (\"'light\", 8),\n",
       " ('appear', 8),\n",
       " ('[', 8),\n",
       " ('Whether', 8),\n",
       " ('printed', 8),\n",
       " ('finish', 8),\n",
       " ('medium', 8),\n",
       " ('them', 7),\n",
       " ('She', 7),\n",
       " ('so', 7),\n",
       " ('ALL', 7),\n",
       " ('pattern', 7),\n",
       " ('perosn', 7),\n",
       " ('oval', 7),\n",
       " ('square', 7),\n",
       " ('4th', 7),\n",
       " ('gender', 7),\n",
       " ('these', 7),\n",
       " ('many', 7),\n",
       " ('gold', 7),\n",
       " ('comb', 7),\n",
       " ('around', 7),\n",
       " ('25', 7),\n",
       " ('yes', 7),\n",
       " ('complexion', 7),\n",
       " ('above', 7),\n",
       " ('Dr', 7),\n",
       " ('specs', 7),\n",
       " ('GUY', 7),\n",
       " ('half', 7),\n",
       " ('stripes', 7),\n",
       " ('height', 7),\n",
       " ('plaid', 7),\n",
       " ('OF', 7),\n",
       " ('knowledge', 7),\n",
       " ('wide', 7),\n",
       " ('ethnicity', 7),\n",
       " ('3rd', 7),\n",
       " ('spiked', 6),\n",
       " ('both', 6),\n",
       " ('heritage', 6),\n",
       " ('goal', 6),\n",
       " ('younger', 6),\n",
       " ('sweatshirt', 6),\n",
       " ('GLASSES', 6),\n",
       " ('use', 6),\n",
       " ('heavy', 6),\n",
       " ('choker', 6),\n",
       " ('open', 6),\n",
       " ('BEST', 6),\n",
       " ('jaw', 6),\n",
       " ('chain', 6),\n",
       " ('braid', 6),\n",
       " ('floral', 6),\n",
       " ('wrinkles', 6),\n",
       " ('Chinese', 6),\n",
       " ('T', 6),\n",
       " ('latino', 6),\n",
       " ('girls', 6),\n",
       " ('THESE', 6),\n",
       " ('race', 6),\n",
       " ('slicked', 6),\n",
       " ('married', 6),\n",
       " ('by', 6),\n",
       " ('go', 6),\n",
       " ('burka', 6),\n",
       " ('sleeves', 6),\n",
       " ('PEOPLE', 6),\n",
       " ('sticking', 6),\n",
       " ('headdress', 6),\n",
       " ('angry', 6),\n",
       " ('PERFORMER', 6),\n",
       " ('5th', 6),\n",
       " ('partially', 6),\n",
       " ('rest', 6),\n",
       " ('ethnically', 6),\n",
       " ('BOY', 6),\n",
       " ('choose', 6),\n",
       " ('turban', 5),\n",
       " ('doe', 5),\n",
       " ('colour', 5),\n",
       " ('put', 5),\n",
       " ('WOMAN', 5),\n",
       " ('narrow', 5),\n",
       " ('HOW', 5),\n",
       " ('using', 5),\n",
       " ('inner', 5),\n",
       " ('This', 5),\n",
       " ('shape', 5),\n",
       " ('DOes', 5),\n",
       " ('seeing', 5),\n",
       " ('perso', 5),\n",
       " ('piece', 5),\n",
       " ('am', 5),\n",
       " ('mark', 5),\n",
       " ('cheeks', 5),\n",
       " ('Hijab', 5),\n",
       " ('was', 5),\n",
       " ('Black', 5),\n",
       " ('earring', 5),\n",
       " ('WHITE', 5),\n",
       " ('world', 5),\n",
       " ('Japanese', 5),\n",
       " ('worn', 5),\n",
       " ('lIfe', 5),\n",
       " ('without', 5),\n",
       " ('2', 5),\n",
       " ('mens', 5),\n",
       " ('braided', 5),\n",
       " ('pigtails', 5),\n",
       " ('8th', 5),\n",
       " ('whats', 5),\n",
       " ('far', 5),\n",
       " ('about', 5),\n",
       " ('How', 5),\n",
       " ('only', 5),\n",
       " ('5', 5),\n",
       " ('eastern', 5),\n",
       " ('Muslim', 5),\n",
       " ('LONG', 5),\n",
       " ('lady', 5),\n",
       " ('beneath', 5),\n",
       " ('little', 5),\n",
       " ('feet', 5),\n",
       " ('All', 5),\n",
       " ('goatee', 5),\n",
       " ('slightly', 5),\n",
       " ('standing', 5),\n",
       " ('cute', 5),\n",
       " ('almost', 4),\n",
       " ('hairline', 4),\n",
       " ('come', 4),\n",
       " ('DARK', 4),\n",
       " ('saying', 4),\n",
       " ('another', 4),\n",
       " ('which', 4),\n",
       " ('Dr.', 4),\n",
       " ('ready', 4),\n",
       " ('way', 4),\n",
       " ('last', 4),\n",
       " ('just', 4),\n",
       " ('question', 4),\n",
       " ('WITH', 4),\n",
       " ('D', 4),\n",
       " ('show', 4),\n",
       " ('looks', 4),\n",
       " ('Her', 4),\n",
       " ('braids', 4),\n",
       " ('then', 4),\n",
       " ('pull', 4),\n",
       " ('south', 4),\n",
       " ('excited', 4),\n",
       " ('UNDER', 4),\n",
       " ('tallest', 4),\n",
       " ('makeup', 4),\n",
       " ('too', 4),\n",
       " ('style', 4),\n",
       " ('hairstyle', 4),\n",
       " ('some', 4),\n",
       " ('haired', 4),\n",
       " ('R', 4),\n",
       " ('spects', 4),\n",
       " ('swept', 4),\n",
       " ('slogan', 4),\n",
       " ('same', 4),\n",
       " ('sdsggsgs', 4),\n",
       " ('stick', 4),\n",
       " ('americans', 4),\n",
       " ('piercings', 4),\n",
       " ('attractive', 4),\n",
       " ('done', 4),\n",
       " ('beautiful', 4),\n",
       " ('IT', 4),\n",
       " ('JOP', 4),\n",
       " ('dress', 4),\n",
       " ('d', 4),\n",
       " ('collard', 4),\n",
       " ('religious', 4),\n",
       " ('SCARF', 4),\n",
       " ('hat', 4),\n",
       " ('teen', 4),\n",
       " ('upper', 4),\n",
       " ('main', 4),\n",
       " ('Did', 4),\n",
       " ('per', 4),\n",
       " ('spanish', 4),\n",
       " ('stripped', 4),\n",
       " ('print', 4),\n",
       " ('smart', 4),\n",
       " ('checkered', 4),\n",
       " ('wears', 4),\n",
       " ('DOE', 4),\n",
       " ('arab', 4),\n",
       " ('considered', 4),\n",
       " ('behind', 4),\n",
       " ('uses', 4),\n",
       " ('photos', 3),\n",
       " ('Male', 3),\n",
       " ('stubble', 3),\n",
       " ('HER', 3),\n",
       " ('yourself', 3),\n",
       " ('gay', 3),\n",
       " ('clearly', 3),\n",
       " ('DO', 3),\n",
       " ('group', 3),\n",
       " ('Person', 3),\n",
       " ('PULLED', 3),\n",
       " ('school', 3),\n",
       " ('kids', 3),\n",
       " ('rosy', 3),\n",
       " ('thier', 3),\n",
       " ('besides', 3),\n",
       " ('WEAR', 3),\n",
       " ('light-colored', 3),\n",
       " ('faced', 3),\n",
       " ('35', 3),\n",
       " ('ugly', 3),\n",
       " ('visable', 3),\n",
       " ('BEARD', 3),\n",
       " ('Have', 3),\n",
       " ('people', 3),\n",
       " ('strange', 3),\n",
       " ('SMART', 3),\n",
       " ('specks', 3),\n",
       " ('gauges', 3),\n",
       " ('origin', 3),\n",
       " ('Impact', 3),\n",
       " ('end', 3),\n",
       " ('neat', 3),\n",
       " ('even', 3),\n",
       " (\"'Life\", 3),\n",
       " ('students', 3),\n",
       " ('being', 3),\n",
       " ('less', 3),\n",
       " ('DID', 3),\n",
       " ('LIGHT', 3),\n",
       " ('remove', 3),\n",
       " ('cloth', 3),\n",
       " ('SHIRT', 3),\n",
       " ('islamic', 3),\n",
       " ('6', 3),\n",
       " ('flowing', 3),\n",
       " ('third', 3),\n",
       " ('dark-colored', 3),\n",
       " ('live', 3),\n",
       " ('vision', 3),\n",
       " ('ninth', 3),\n",
       " ('buzzcut', 3),\n",
       " ('ca', 3),\n",
       " ('earings', 3),\n",
       " ('AN', 3),\n",
       " ('bowl', 3),\n",
       " ('SHORT', 3),\n",
       " ('lower', 3),\n",
       " ('His', 3),\n",
       " ('Hispanic', 3),\n",
       " ('Doe', 3),\n",
       " ('answer', 3),\n",
       " ('golden', 3),\n",
       " ('hoop', 3),\n",
       " ('spikey', 3),\n",
       " ('tight', 3),\n",
       " ('framing', 3),\n",
       " ('TALL', 3),\n",
       " ('scraf', 3),\n",
       " ('let', 3),\n",
       " ('WHAT', 3),\n",
       " ('now', 3),\n",
       " ('frizzy', 3),\n",
       " ('CURLY', 3),\n",
       " ('type', 3),\n",
       " ('shaggy', 3),\n",
       " ('football', 3),\n",
       " ('YOUNG', 3),\n",
       " ('brushed', 3),\n",
       " ('doing', 3),\n",
       " ('click', 3),\n",
       " ('shoulder-length', 3),\n",
       " ('FAT', 3),\n",
       " (\"'m\", 3),\n",
       " ('aged', 3),\n",
       " ('arm', 3),\n",
       " ('completion', 3),\n",
       " ('colorful', 3),\n",
       " ('logo', 3),\n",
       " ('IN', 3),\n",
       " ('completely', 3),\n",
       " ('surprised', 3),\n",
       " ('modern', 3),\n",
       " ('7th', 3),\n",
       " ('everyone', 3),\n",
       " ('straightly', 3),\n",
       " ('set', 3),\n",
       " ('bushy', 3),\n",
       " ('MEMBERS', 3),\n",
       " ('dark-skinned', 3),\n",
       " ('split', 3),\n",
       " ('HEAD', 3),\n",
       " ('redhead', 3),\n",
       " ('hizab', 3),\n",
       " ('touch', 3),\n",
       " ('away', 3),\n",
       " (\"n't\", 3),\n",
       " ('speak', 3),\n",
       " ('maths', 3),\n",
       " ('WORK', 3),\n",
       " ('figure', 3),\n",
       " ('mans', 3),\n",
       " ('ha', 3),\n",
       " ('NAME', 3),\n",
       " ('class', 2),\n",
       " ('clicked', 2),\n",
       " ('Yellow', 2),\n",
       " ('drinks', 2),\n",
       " ('multiple', 2),\n",
       " ('They', 2),\n",
       " ('sitting', 2),\n",
       " ('mole', 2),\n",
       " ('dimples', 2),\n",
       " ('list', 2),\n",
       " ('place', 2),\n",
       " ('closed', 2),\n",
       " ('complected', 2),\n",
       " ('longish', 2),\n",
       " ('bit', 2),\n",
       " ('frame', 2),\n",
       " ('GAME', 2),\n",
       " ('rather', 2),\n",
       " ('BLOND', 2),\n",
       " ('moustach', 2),\n",
       " ('dos', 2),\n",
       " ('muscular', 2),\n",
       " ('relationship', 2),\n",
       " ('iis', 2),\n",
       " ('dimple', 2),\n",
       " ('slender', 2),\n",
       " ('guilty', 2),\n",
       " ('PLAYING', 2),\n",
       " ('fit', 2),\n",
       " ('natural', 2),\n",
       " ('BLUE', 2),\n",
       " ('asain', 2),\n",
       " ('u', 2),\n",
       " ('want', 2),\n",
       " ('frames', 2),\n",
       " ('seem', 2),\n",
       " ('giving', 2),\n",
       " ('identified', 2),\n",
       " ('BLACK', 2),\n",
       " ('mustaqh', 2),\n",
       " ('Am', 2),\n",
       " ('infoasnfgosa', 2),\n",
       " ('hait', 2),\n",
       " ('came', 2),\n",
       " ('e', 2),\n",
       " ('hide', 2),\n",
       " ('touching', 2),\n",
       " ('fashioned', 2),\n",
       " ('me', 2),\n",
       " ('UP', 2),\n",
       " ('bigger', 2),\n",
       " ('deep', 2),\n",
       " ('COME', 2),\n",
       " ('into', 2),\n",
       " ('india', 2),\n",
       " ('Hitler', 2),\n",
       " ('forward', 2),\n",
       " ('headwear', 2),\n",
       " ('handsome', 2),\n",
       " ('traveling', 2),\n",
       " ('broad', 2),\n",
       " ('else', 2),\n",
       " ('SILENT', 2),\n",
       " ('50', 2),\n",
       " ('WHERE', 2),\n",
       " ('har', 2),\n",
       " ('contain', 2),\n",
       " ('Glasses', 2),\n",
       " ('when', 2),\n",
       " ('ponytails', 2),\n",
       " ('teenager', 2),\n",
       " ('least', 2),\n",
       " ('gathered', 2),\n",
       " ('bang', 2),\n",
       " ('reads', 2),\n",
       " ('1', 2),\n",
       " ('fluffy', 2),\n",
       " ('tshirts', 2),\n",
       " ('clothes', 2),\n",
       " ('loves', 2),\n",
       " ('petite', 2),\n",
       " ('swim', 2),\n",
       " ('Anglo', 2),\n",
       " ('crew', 2),\n",
       " ('unique', 2),\n",
       " ('scared', 2),\n",
       " ('INDIA', 2),\n",
       " (\"'The\", 2),\n",
       " ('going', 2),\n",
       " ('piercing', 2),\n",
       " ('RAPE', 2),\n",
       " ('uncombed', 2),\n",
       " ('taking', 2),\n",
       " ('Wearing', 2),\n",
       " ('vote', 2),\n",
       " ('smiley', 2),\n",
       " ('Which', 2),\n",
       " ('CHINA', 2),\n",
       " ('yellowish', 2),\n",
       " ('alcohol', 2),\n",
       " ('infront', 2),\n",
       " ('five', 2),\n",
       " ('neatly', 2),\n",
       " ('doies', 2),\n",
       " ('ever', 2),\n",
       " ('UNDERSHIRT', 2),\n",
       " ('20', 2),\n",
       " ('buzz', 2),\n",
       " ('college', 2),\n",
       " ('fine', 2),\n",
       " ('hav', 2),\n",
       " ('fully', 2),\n",
       " ('happen', 2),\n",
       " ('features', 2),\n",
       " ('anyone', 2),\n",
       " ('posture', 2),\n",
       " ('shadow', 2),\n",
       " ('DOWN', 2),\n",
       " ('BLONDE', 2),\n",
       " ('ok', 2),\n",
       " ('tired', 2),\n",
       " ('MURDER', 2),\n",
       " ('skarf', 2),\n",
       " ('but', 2),\n",
       " ('button', 2),\n",
       " ('YELLOW', 2),\n",
       " ('ethnic', 2),\n",
       " ('serious', 2),\n",
       " ('always', 2),\n",
       " ('stand', 2),\n",
       " ('flower', 2),\n",
       " ('pig', 2),\n",
       " ('hijaab', 2),\n",
       " ('bright', 2),\n",
       " ('fake', 2),\n",
       " ('pony-tail', 2),\n",
       " ('oldest', 2),\n",
       " ('headress', 2),\n",
       " ('much', 2),\n",
       " ('headpiece', 2),\n",
       " ('FROM', 2),\n",
       " ('formal', 2),\n",
       " ('keeping', 2),\n",
       " ('slanted', 2),\n",
       " ('impairment', 2),\n",
       " ('brows', 2),\n",
       " ('exposed', 2),\n",
       " (\"'THE\", 2),\n",
       " ('tails', 2),\n",
       " ('section', 2),\n",
       " ('shortest', 2),\n",
       " ('expression', 2),\n",
       " ('year', 2),\n",
       " ('FAMILY', 2),\n",
       " ('made', 2),\n",
       " ('music', 2),\n",
       " ('mustach', 2),\n",
       " ('different', 2),\n",
       " ('na', 2),\n",
       " ('whole', 2),\n",
       " ('celebrity', 2),\n",
       " ('extremely', 2),\n",
       " ('Mustache', 2),\n",
       " ('weird', 2),\n",
       " ('BEAUTY', 2),\n",
       " ('consider', 2),\n",
       " ('delicate', 2),\n",
       " ('remaining', 2),\n",
       " ('persona', 2),\n",
       " ('styled', 2),\n",
       " ('slight', 2),\n",
       " ('asia', 2),\n",
       " ('T-shirt', 2),\n",
       " ('japanese', 2),\n",
       " ('latin', 2),\n",
       " ('spectacular', 2),\n",
       " ('CRIMINAL', 2),\n",
       " ('missed', 2),\n",
       " ('multi-colored', 2),\n",
       " ('stripe', 2),\n",
       " ('corner', 2),\n",
       " ('shaped', 2),\n",
       " ('In', 2),\n",
       " ('statue', 2),\n",
       " ('teh', 2),\n",
       " ('FACE', 2),\n",
       " ('wore', 2),\n",
       " ('brow', 2),\n",
       " ('Anyone', 2),\n",
       " ('labeled', 2),\n",
       " ('normal', 2),\n",
       " ('ido', 2),\n",
       " ('eyebrow', 2),\n",
       " ('GROUP', 2),\n",
       " ('seen', 2),\n",
       " ('whiter', 2),\n",
       " ('mostly', 2),\n",
       " ('weight', 2),\n",
       " ('laying', 2),\n",
       " ('spot', 2),\n",
       " ('SHE', 2),\n",
       " ('listen', 2),\n",
       " ('dye', 2),\n",
       " ('receeding', 2),\n",
       " ('CRIME', 2),\n",
       " ('faces', 2),\n",
       " ('somewhat', 2),\n",
       " ('r', 2),\n",
       " ('handed', 2),\n",
       " ('galss', 2),\n",
       " ('obviously', 2),\n",
       " ('bobtail', 2),\n",
       " ('unnatural', 2),\n",
       " ('poking', 2),\n",
       " ('veil', 2),\n",
       " ('heavier', 2),\n",
       " ('BACK', 2),\n",
       " ('somewhere', 2),\n",
       " ('together', 2),\n",
       " ('north', 1),\n",
       " ('BANK', 1),\n",
       " ('whatever', 1),\n",
       " ('ring', 1),\n",
       " ('nidfafff', 1),\n",
       " ('thrd', 1),\n",
       " ('draped', 1),\n",
       " ('shouldars', 1),\n",
       " ('chair', 1),\n",
       " ('screen', 1),\n",
       " ('womens', 1),\n",
       " ('skined', 1),\n",
       " ('marker', 1),\n",
       " ('FOREIG', 1),\n",
       " ('KNOW', 1),\n",
       " ('intelligent', 1),\n",
       " ('havea', 1),\n",
       " ('student', 1),\n",
       " ('politician', 1),\n",
       " ('tooth', 1),\n",
       " ('fifth', 1),\n",
       " ('smirking', 1),\n",
       " ('BEARED', 1),\n",
       " ('sports', 1),\n",
       " (\"'alternative\", 1),\n",
       " ('fauxhawk', 1),\n",
       " ('VIOLENCE', 1),\n",
       " ('highlights', 1),\n",
       " ('blush', 1),\n",
       " ('teacher', 1),\n",
       " ('medication', 1),\n",
       " ('font', 1),\n",
       " ('workforce', 1),\n",
       " ('center', 1),\n",
       " ('ORGANIZATION', 1),\n",
       " ('>', 1),\n",
       " ('Long', 1),\n",
       " ('dead', 1),\n",
       " ('abald', 1),\n",
       " ('COVERING', 1),\n",
       " ('tanned', 1),\n",
       " ('wieght', 1),\n",
       " ('love', 1),\n",
       " ('copy', 1),\n",
       " ('box', 1),\n",
       " ('aqnfmpsadfg', 1),\n",
       " ('shadowed', 1),\n",
       " ('personality', 1),\n",
       " ('SKINNY', 1),\n",
       " ('trying', 1),\n",
       " ('touches', 1),\n",
       " ('drew', 1),\n",
       " ('china', 1),\n",
       " ('church', 1),\n",
       " ('boys', 1),\n",
       " ('mowhawk', 1),\n",
       " ('close', 1),\n",
       " ('MARRIED', 1),\n",
       " ('hang', 1),\n",
       " ('speaking', 1),\n",
       " ('ask', 1),\n",
       " ('oops', 1),\n",
       " ('shown', 1),\n",
       " ('if', 1),\n",
       " ('SIDE', 1),\n",
       " ('raised', 1),\n",
       " ('ignored', 1),\n",
       " ('Doest', 1),\n",
       " ('MANY', 1),\n",
       " ('femael', 1),\n",
       " ('bread', 1),\n",
       " ('GETTING', 1),\n",
       " ('CONFUSE', 1),\n",
       " ('running', 1),\n",
       " ('sandy', 1),\n",
       " ('circular', 1),\n",
       " ('allot', 1),\n",
       " ('glais', 1),\n",
       " ('navy', 1),\n",
       " ('button-down', 1),\n",
       " ('hi', 1),\n",
       " ('Dark', 1),\n",
       " ('few', 1),\n",
       " ('curls', 1),\n",
       " ('corrective', 1),\n",
       " ('america', 1),\n",
       " ('ALCOHOL', 1),\n",
       " ('dresses', 1),\n",
       " ('WEIT', 1),\n",
       " ('white/caucasian', 1),\n",
       " ('da', 1),\n",
       " ('across', 1),\n",
       " ('irfe', 1),\n",
       " ('three', 1),\n",
       " ('mose', 1),\n",
       " ('code', 1),\n",
       " ('neutral', 1),\n",
       " ('trimmed', 1),\n",
       " ('our', 1),\n",
       " ('Spanish', 1),\n",
       " ('win', 1),\n",
       " ('STUDENTS', 1),\n",
       " ('obese', 1),\n",
       " ('real', 1),\n",
       " ('trump', 1),\n",
       " ('BRAVE', 1),\n",
       " ('afaonfoafn', 1),\n",
       " ('identifying', 1),\n",
       " ('THEIR', 1),\n",
       " ('Resizing', 1),\n",
       " ('LOT', 1),\n",
       " ('coming', 1),\n",
       " ('females', 1),\n",
       " ('asian=', 1),\n",
       " ('potato', 1),\n",
       " ('wrong', 1),\n",
       " ('solve', 1),\n",
       " ('Dose', 1),\n",
       " ('faith', 1),\n",
       " ('thet', 1),\n",
       " ('LAZY', 1),\n",
       " ('desent', 1),\n",
       " ('messed', 1),\n",
       " ('goes', 1),\n",
       " ('bra', 1),\n",
       " ('sccarf', 1),\n",
       " ('bandana', 1),\n",
       " ('laid', 1),\n",
       " ('unselect', 1),\n",
       " ('still', 1),\n",
       " ('ASSULT', 1),\n",
       " ('get', 1),\n",
       " ('Burnette', 1),\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fd1 = nltk.FreqDist(new_table_fd[0:20])\n",
    "fd1 = nltk.FreqDist(new_table_fd)\n",
    "fd1.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(all_i_txt[0])\n",
    "##basic model\n",
    "tf = TfidfVectorizer(ngram_range = (2,3), sublinear_tf = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_sim_tf = []\n",
    "for l in range(len(all_i_txt)):\n",
    "    H = tf.fit_transform(all_i_txt[l])\n",
    "    sim = cosine_similarity(H)\n",
    "    all_sim_tf.append(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14644\n"
     ]
    }
   ],
   "source": [
    "#similarities[-2] meaning taking only the array for question\n",
    "all_sim_q_tf = []\n",
    "for i in range(len(all_sim_tf)):\n",
    "    all_sim_q_tf.append(all_sim_tf[i][-2][:-2])\n",
    "print(len(all_sim_q_tf))\n",
    "#print(all_sim_q[971])\n",
    "#print(len(all_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Sort all the above similarities with argsort which gives indices\n",
    "## or we should not take top5 and make a threshold above which we will include it in the list of output?\n",
    "sorted_ind_tf = []\n",
    "for si in all_sim_q_tf:\n",
    "    y_top = si.argsort()[::-1]\n",
    "    sorted_ind_tf.append(y_top)\n",
    "#sorted_ind[971]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14644\n"
     ]
    }
   ],
   "source": [
    "sorted_sim_all_tf = []\n",
    "for q,w in zip(sorted_ind_tf, all_sim_q_tf):\n",
    "    sorted_sim = []\n",
    "    for e in q:\n",
    "        sorted_sim.append(w[e])\n",
    "    sorted_sim_all_tf.append(sorted_sim)\n",
    "print(len(sorted_sim_all_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14644"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##find the difference between two similarity score in each array.\n",
    "sim_dif_all_tf = []\n",
    "\n",
    "for d in range(len(sorted_sim_all_tf)):\n",
    "    sim_dif = [sorted_sim_all_tf[d][f-1] - sorted_sim_all_tf[d][f] for f in range(1, len(sorted_sim_all_tf[d]))]\n",
    "    sim_dif_all_tf.append(sim_dif)\n",
    "        \n",
    "len(sim_dif_all_tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14644\n",
      "14644\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_dif_tf = []\n",
    "max_dif_ind_tf =[]\n",
    "for g in range(len(sim_dif_all_tf)):\n",
    "    \n",
    "    maxi= max(sim_dif_all_tf[g])\n",
    "    max_dif_tf.append(maxi)\n",
    "    max_dif_ind_tf.append(sim_dif_all_tf[g].index(maxi))\n",
    "print(len(max_dif_tf))\n",
    "print(len(max_dif_ind_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_ind_lim_tf = [indi[0:max_indi+1] for indi,max_indi in zip(sorted_ind_tf, max_dif_ind_tf)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##clean_all_x\n",
    "all_x_clean = [all_x[i] for i in original_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_img = zip(all_x_clean, sorted_ind_lim_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## convert to img_id from index to match with actual image ids selcted by participant\n",
    "def mk_imgid(arr):\n",
    "    y_pred = []\n",
    "    for i,j in arr:\n",
    "        yp = []\n",
    "        for s in j:\n",
    "            yp.append(i[s])\n",
    "        y_pred.append(yp)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_img = mk_imgid(top_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IMG_9048']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_img)\n",
    "pred_img[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_cheat1 = mk_list(fe['curr_elim'])\n",
    "all_cheat = [all_cheat1[i] for i in original_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IMG_9634', 'IMG_7942']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_cheat)\n",
    "all_cheat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14644\n",
      "14644\n"
     ]
    }
   ],
   "source": [
    "pre_all= [len(set(a).intersection(set(p)))/len(set(p)) if len(p) > 0 else 0 for a,p in zip(all_cheat, pred_img)]\n",
    "rec_all = [len(set(a).intersection(set(p)))/len(set(a)) if len(a) > 0 else 0 for a,p in zip(all_cheat, pred_img)]\n",
    "print(len(pre_all))\n",
    "print(len(rec_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14644"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs_all= [(0.5*0.5 + 1)*pr*re/ (0.5*0.5*pr + re) if pr > 0 or re >0 else 0 for pr, re in zip(pre_all, rec_all)]\n",
    "len(fs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4256663707548029"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_mean = np.mean(np.array(pre_all))\n",
    "rec_mean = np.mean(np.array(rec_all))\n",
    "fs_mean = np.mean(np.array(fs_all))\n",
    "pre_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open('Results with description.txt', 'w') as f:\n",
    "#    f.write(\"No., Model, Parameter, Decision Rules, Precision, Recall, F0.5\"+ '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "op3 = [\"17\", \"TFIDF, NLTK, POS\", \"ngram_range = 2,3, sublineartf true\", \"Cosinesimilarity and ranking difference\",str(pre_mean), str(rec_mean), str(fs_mean)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Results with description.txt', 'a') as f:\n",
    "    f.write(', '.join(op3)+ '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## all similarities\n",
    "all_sim_cv = []\n",
    "for l in range(len(all_i_txt)):\n",
    "    H = cv.fit_transform(all_i_txt[l])\n",
    "    sim = cosine_similarity(H)\n",
    "    all_sim_cv.append(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14644\n"
     ]
    }
   ],
   "source": [
    "#similarities[-2] meaning taking only the array for question\n",
    "all_sim_q_cv = []\n",
    "for i in range(len(all_sim_cv)):\n",
    "    all_sim_q_cv.append(all_sim_cv[i][-2][:-2])\n",
    "print(len(all_sim_q_cv))\n",
    "#print(all_sim_q[971])\n",
    "#print(len(all_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Sort all the above similarities with argsort which gives indices\n",
    "## or we should not take top5 and make a threshold above which we will include it in the list of output?\n",
    "sorted_ind_cv = []\n",
    "for si in all_sim_q_cv:\n",
    "    y_top = si.argsort()[::-1]\n",
    "    sorted_ind_cv.append(y_top)\n",
    "#sorted_ind[971]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14644\n"
     ]
    }
   ],
   "source": [
    "sorted_sim_all_cv = []\n",
    "for q,w in zip(sorted_ind_cv, all_sim_q_cv):\n",
    "    sorted_sim = []\n",
    "    for e in q:\n",
    "        sorted_sim.append(w[e])\n",
    "    sorted_sim_all_cv.append(sorted_sim)\n",
    "print(len(sorted_sim_all_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14644"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##find the difference between two similarity score in each array.\n",
    "sim_dif_all_cv = []\n",
    "\n",
    "for d in range(len(sorted_sim_all_cv)):\n",
    "    sim_dif = [sorted_sim_all_cv[d][f-1] - sorted_sim_all_cv[d][f] for f in range(1, len(sorted_sim_all_cv[d]))]\n",
    "    sim_dif_all_cv.append(sim_dif)\n",
    "        \n",
    "len(sim_dif_all_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14644\n",
      "14644\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_dif_cv = []\n",
    "max_dif_ind_cv =[]\n",
    "for g in range(len(sim_dif_all_cv)):\n",
    "    \n",
    "    maxi= max(sim_dif_all_cv[g])\n",
    "    max_dif_cv.append(maxi)\n",
    "    max_dif_ind_cv.append(sim_dif_all_cv[g].index(maxi))\n",
    "print(len(max_dif_cv))\n",
    "print(len(max_dif_ind_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_ind_lim_cv = [indi[0:max_indi+1] for indi,max_indi in zip(sorted_ind_cv, max_dif_ind_cv)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_img_cv = zip(all_x_clean, sorted_ind_lim_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_img_cv = mk_imgid(top_img_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14644\n",
      "14644\n"
     ]
    }
   ],
   "source": [
    "pre_all_cv= [len(set(a).intersection(set(p)))/len(set(p)) if len(p) > 0 else 0 for a,p in zip(all_cheat, pred_img_cv)]\n",
    "rec_all_cv = [len(set(a).intersection(set(p)))/len(set(a)) if len(a) > 0 else 0 for a,p in zip(all_cheat, pred_img_cv)]\n",
    "print(len(pre_all_cv))\n",
    "print(len(rec_all_cv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14644"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs_all_cv= [(0.5*0.5 + 1)*pr*re/ (0.5*0.5*pr + re) if pr > 0 or re >0 else 0 for pr, re in zip(pre_all_cv, rec_all_cv)]\n",
    "len(fs_all_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.031238461491466137"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_mean_cv = np.mean(np.array(pre_all_cv))\n",
    "rec_mean_cv = np.mean(np.array(rec_all_cv))\n",
    "fs_mean_cv = np.mean(np.array(fs_all_cv))\n",
    "pre_mean_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "op3 = [\"18\", \"cv, NLTK, POS\", \"default\", \"Cosinesimilarity and ranking difference\",str(pre_mean_cv), str(rec_mean_cv), str(fs_mean_cv)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Results with description.txt', 'a') as f:\n",
    "    f.write(', '.join(op3)+ '\\n')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
